---
title: "cat2cat - Introduction"
author: "Maciej Nasinski"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{miceFast - Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(size = "tiny")
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
```

```{r}
options(scipen = 999)
pacman::p_load(cat2cat, dplyr)
```

# Mapping of a categorical variable in a panel dataset according to a new encoding

This document is still in development

**The main rule is to replicate the observation if it could be assign to a few categories**
**then using simple freqencies or ml model to approximate probabilities of being assign to each of them.**

Why cat2cat:  
- universal algorithm which could be used in different science fields  
- stop removing variables for ml models because variable categories are not the same across time  
- use a statistical modelling to join datasets from different time points and retain caterogical variable structure  
- visualize any factor variable across time  

In many projects where dataset contains a categorical variable one of the biggest obstacle is that 
the data provider during internal processes was changing an encoding of this variable during a time.
Thus some categories were grouped and other were separated or a new one is added or an old one is removed.

### Different types of panel data
## panel dataset without unique identifier

We could not observe what category have each observations across time.
There is not a possibility to build automatically a mapping.
We have to provide manually a mapping table - usually by a data provider like a national statistics office.
In worst case somebody have to make it manually though supported by some officials directive.

ID, name, code, date  
uninformative anonymous carpenter 102 2020-02-10  
uninformative anonymous plumber 103 2020-02-10  
uninformative anonymous carpenter or plumber 121 2020-02-12  
uninformative anonymous carpenter or plumber 121 2020-02-12  

We want to map it 
Here we artificially enlarge degrees of freedom  

 carpenter   102  2020-02-10  
 plumber  103 2020-02-10  
 carpenter 102  2020-02-12 50%  
 plumber 103 2020-02-12 50%  
 carpenter 102  2020-02-12 50%  
 plumber 103 2020-02-12 50%  
 
or

carpenter or plumber  121 2020-02-10  50%  
carpenter or plumber  121 2020-02-10  50%  
carpenter or plumber 121 2020-02-10 50%  
carpenter or plumber 121 2020-02-10 50%  
carpenter or plumber 121 2020-02-12   
carpenter or plumber  121 2020-02-12 
 
## panel dataset with unique identifier

Here we could observe what category have each subject across time
There is a possibility to build a mapping automatically

we could have new identifiers only in newest partition ...  
or old identifiers which do not continue to participate in abandoned group

000 Retire Jim carpenter 102 2020-02-10
111 John Doe carpenter 102 2020-02-10  
222 Jimmy Smith plumber 103 2020-02-10  
111 John Doe carpenter or plumber 121 2020-02-12  
222 Jimmy Smith carpenter or plumber 121 2020-02-12  

103,102 -> 121

Here we want to adjust to a new encoding  
000 Retire Jim carpenter or plumber 121 2020-02-10 some%
111 John Doe carpenter or plumber  121 2020-02-10  
222 Jimmy Smith carpenter or plumber  121 2020-02-10  
111 John Doe carpenter or plumber 121 2020-02-12  
222 Jimmy Smith carpenter or plumber 121 2020-02-12  

Here we adjust to old encoding if we are sure that subjects have the same class.
We have to assume that each person could not change the occupation which sometimes might seems not reasonable.

111 John Doe carpenter 102 2020-02-10  
222 Jimmy Smith plumber 103 2020-02-10  
111 John Doe carpenter 102 2020-02-12  
222 Jimmy Smith carpenter 103 2020-02-12  

Thus the best solutioon could be build a transition table and use algoritm for dataset without identifier.

### simple frequencies vs ML module 

The main objective is to merge X surveys published by Y over the years.
We could get a few types of data:

ML unsupervised to choose most probable categories, narrow number of possible categories

#### Simple frequencies

The first hash table - transitions:

```
> head(...)
$`1111`
[1] "1111"
$`1112`
[1] "1112"
$`1113`
[1] "1112"
$`1114`
[1] "1121" "1122" "1123"
$`1120`
[1] "1211" "1212"
$`1311`
[1] "1221" "1311"
```

However each new observation has some probability of existence because of the fact that it could be assign to a few groups.
Thus we built a next hash table which provide probabilities of attendance in each group. 
Probabilities were calculated using frequencies of people in each group at the previous survey. 
This should be obvious that for each observations which was replicated a few times probabilities have to sum to one. 
There were made some assumptions about cases such as when there was no the same group in the previous survey.

The second hash table - probabilities:

```
> head(cat_apply_freq(...) 
$`1111`                                        
[1] 1
$`1112`
[1] 1
$`1113`
[1] 1
$`1114`
[1] 0.3333333 0.3333333 0.3333333
$`1120`
[1] 0.8343979 0.1656021
$`1311`
[1] 0.95471934 0.04528066
```

Finally we get the dataset which has additional rows from a replication process and 3 supplementary variables - new group,probability and number of replications.

Data should be preprocessed before building a regression model because of a high complexity. 
We implemented a deep explanatory analysis to understand data characteristics so right preprocessing decisions could be chosen. 

The extended Mincer equation was used to estimate this relation. 
Logarithmic wages was regressed against individual characteristics relevant from the perspective of the labour market

#### ML module



\newpage

## Data

###

occup dataset is an example of unbalance panel dataset.
It is presenting a characteristics from randomly selected company and then using k step procedure employees are chosen. 
The survey is anonymous and take place every two years. Use ?occup for more details.

```{r}
data(occup)

data(trans)
```

```{r}
occup %>% glimpse()
```

trans dataset containing transitions between old (2008) and new (2010) occupational codes.

```{r}
trans %>% glimpse()
```

\newpage

### Mappings

If you are interested in the utils functions here are presented some 

```{r}
mappings <- get_mappings(trans)

mappings$to_old[1:4]

mappings$to_new[1:4]

mapp_p <- cat_apply_freq(mappings$to_old, get_freqs(occup$code4[occup$year == "2008"], occup$multipier[occup$year == "2008"]))

data.frame(I(mappings$to_old), I(mapp_p)) %>% head()

mapp_p <- cat_apply_freq(mappings$to_new, get_freqs(occup$code4[occup$year == "2010"], occup$multipier[occup$year == "2010"]))

data.frame(I(mappings$to_new), I(mapp_p)) %>% head()
```

\newpage

### cat2cat applied for occup data panel

splitting the data

```{r}
occup_old = occup[occup$year == 2008,]
occup_new = occup[occup$year == 2010,]
```

simple model where probabilities will be taken from a category frequency.

```{r}
cat2cat(
  data = list(old = occup_old ,new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "forward")
  )
```

### ml module

Currently only knn method is available.

```{r}
occup_2 = cat2cat(
  data = list(old = occup_old ,new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "forward")
  ,ml = list(method = "knn", features = c("age", "sex", "edu", "exp", "parttime", "salary"), args = list(k = 10))
  )
```

```{r}
occup_2 %>% glimpse()
```


```{r, size="tiny"}
occup_3 <- cat2cat(
  data = list(old = occup_old, new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "backward"),
  ml = list(method = "knn", features = c("age", "sex", "edu", "exp", "parttime", "salary"), args = list(k = 10))
)
```


notice : if the ml model is failing then NA is returned so 

cor(occup_3$old$wei_ml_c2c, occup_3$old$wei_freq_c2c, use = "complete.obs")


```{r}
occup_3 %>% glimpse()
```

New freqencies for potential next step:

```{r}
get_freqs(x = occup_3$new$g_new_c2c, multipier = floor(occup_3$new$multipier * occup_3$new$wei_freq_c2c)) %>% head()
```

or backward:

```{r}
get_freqs(x = occup_2$old$g_new_c2c, multipier = floor(occup_2$old$multipier * occup_2$old$wei_freq_c2c))%>% head()
```

\newpage

### Example - linear regressions:

Remember to correct degrees of freedom


### providing mappings manually

example of aggregate dataset where we want to specify transitions manually:

Simulate the data.

```{r}
set.seed(1234)

agg_old <- data.frame(
  vertical = c("Electronics", "Kids1", "Kids2", "Automotive", "Books", "Clothes", "Home", "Fashion", "Health", "Sport"),
  sales = rnorm(10, 100, 10),
  counts = rgeom(10, 0.0001),
  v_date = rep("2020-04-01", 10), stringsAsFactors = F
)

agg_new <- data.frame(
  vertical = c("Electronics", "Supermarket", "Kids", "Automotive1", "Automotive2", "Books", "Clothes", "Home", "Fashion", "Health", "Sport"),
  sales = rnorm(11, 100, 10),
  counts = rgeom(11, 0.0001),
  v_date = rep("2020-05-01", 11), stringsAsFactors = F
)
```

```{r}
agg_old %>% glimpse()
```

```{r}
agg_old %>% glimpse()
```


Remember that after that you have to aggragate the results in a proper way. 

Moreover each analysis could requires specific aggregation process.


```{r}
# old -> new,new, old-> old,new
agg1 = cat2cat_agg(data = list(old = agg_old, new = agg_new, cat_var = "vertical", time_var = "v_date",freq_var = "counts"),
            Automotive %>% c(Automotive1, Automotive2), c(Kids1, Kids2) %>% c(Kids), Home %>% c(Home, Supermarket))

agg1$old
```

```{r}
agg1$new %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))
```

changes in both direction.

```{r}
agg2 = cat2cat_agg(data = list(old = agg_old, new = agg_new, cat_var = "vertical", time_var = "v_date", freq_var = "counts"), 
            Automotive %<% c(Automotive1, Automotive2), c(Kids1, Kids2) %>% c(Kids), Home %>% c(Home, Supermarket))
```

```{r}
agg2$old %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))

agg2$new %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))

```

# what if there are a many time points to map

then cat2cat have to be used recursively where the prune method might be needed to limit the problem of exponentially growing replications.
Thus a prune_c2c with highest1 or highest method could be used before setting a df to next iteration.
However when only 3 periods have to be map then the middle one might be the base one.


