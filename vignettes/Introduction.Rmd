---
title: "cat2cat - Introduction"
author: "Maciej Nasinski"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{miceFast - Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
knitr::opts_chunk$set(size = "tiny")
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
```

```{r}
options(scipen = 999)
pacman::p_load(cat2cat, dplyr)
```

# IN DEVELOPMENT - transform a categorical variable according to a new encoding

IN DEVELOPMENT
IN DEVELOPMENT
IN DEVELOPMENT
IN DEVELOPMENT
IN DEVELOPMENT

In many projects where dataset contains a caterogical variable one of the biggest obstacle is that 
the data provider during internal proceesses was changing an encoding of this variable during a time.

Thus some categories were grouped and other were separated or a new one is added or an old one is removed.

There should be stated a 3 clear questions:

1. Do i have a transition table. 
2. Type of the data - panel balanced or unbalanced or aggregate data vs individual data.
3. Direction of a transition, forward or backward - use a new or an old encoding

## panel unbalanced

Here we could not observe what category have each observations across time:
There is not a possibility to build automatically mapping
We have to provide manually a mapping table - usually by a data provider like a country statistics office
In worst case made it manually when supported by some officials directive.

ID, name, code, date  
uninformative anonymous carpenter 102 2020-02-10  
uninformative anonymous plumber 103 2020-02-10  
uninformative anonymous carpenter or plumber 121 2020-02-12  
uninformative anonymous carpenter or plumber 121 2020-02-12  

Here we artificially enlarge degrees of freedom  
 carpenter   121 2020-02-10  
 plumber  121 2020-02-10  
 carpenter 121 2020-02-12 50%  
 plumber 121 2020-02-12 50%  
 carpenter 121 2020-02-12 50%  
 plumber  121 2020-02-12 50%  
 
## panel balanced

Here we could observe what category have each observation across time:
There is a possibility to build a mapping automatically

we could have new identifiers only in newest partition ...  
or old identifiers which do not continue to participate in abandoned group

111 John Doe carpenter 102 2020-02-10  
222 Jimmy Smith plumber 103 2020-02-10  
111 John Doe carpenter or plumber 121 2020-02-12  
222 Jimmy Smith carpenter or plumber 121 2020-02-12  

103,102 -> 121

Here we want to adjust to a new encoding  
111 John Doe carpenter or plumber  121 2020-02-10  
222 Jimmy Smith carpenter or plumber  121 2020-02-10  
111 John Doe carpenter or plumber 121 2020-02-12  
222 Jimmy Smith carpenter or plumber 121 2020-02-12  

Here we adjust to old encodingif we are sure that subjects have the same class.
We have to assume that each person could not chenge the occupation which seems not reasonable.
This assumption could be satste in other direction too for other example.

111 John Doe carpenter 102 2020-02-10  
222 Jimmy Smith plumber 103 2020-02-10  
111 John Doe carpenter 102 2020-02-12  
222 Jimmy Smith carpenter 103 2020-02-12  

Thus the best solutioon could be build a transition table and use algoritm for ubalance data.

### simple frequencies vs ML module 

The main objective is to merge X surveys published by Y over the years.
We could get a few types of data:

ML unsupervised to choose most probable categories, narrow number of possible categories

#### Simple freqencies

The first hash table - transitions:

```
> head(...)
$`1111`
[1] "1111"
$`1112`
[1] "1112"
$`1113`
[1] "1112"
$`1114`
[1] "1121" "1122" "1123"
$`1120`
[1] "1211" "1212"
$`1311`
[1] "1221" "1311"
```

However each new observation has some probability of existence because of the fact that it could be assign to a few groups.
Thus we built a next hash table which provide probabilities of attendance in each group. 
Probabilities were calculated using frequencies of people in each group at the previous survey. 
This should be obvious that for each observations which was replicated a few times probabilities have to sum to one. 
There were made some assumptions about cases such as when there was no the same group in the previous survey.

The second hash table - probabilities:

```
> head(cat_apply_freq(...) 
$`1111`                                        
[1] 1
$`1112`
[1] 1
$`1113`
[1] 1
$`1114`
[1] 0.3333333 0.3333333 0.3333333
$`1120`
[1] 0.8343979 0.1656021
$`1311`
[1] 0.95471934 0.04528066
```

Finally we get the dataset which has additional rows from a replication process and 3 supplementary variables - new group,probability and number of replications.

Data should be preprocessed before building a regression model because of a high complexity. 
We implemented a deep explanatory analysis to understand data characteristics so right preprocessing decisions could be chosen. 

The extended Mincer equation was used to estimate this relation. 
Logarithmic wages was regressed against individual characteristics relevant from the perspective of the labour market

#### ML module



\newpage

## Data

occup dataset is an example of unbalance panel dataset.
It is presenting a characteristics from randomly selected company and then using k step procedure employees are chosen. 
The survey is anonymous and take place every two years. Use ?occup for more details.

```{r}
data(occup)

data(trans)
```

```{r}
occup %>% glimpse()
```


trans dataset containing transitions between old (2008) and new (2010) occupational codes.

```{r}
trans %>% glimpse()
```

\newpage

### Mappings

If you are interested in the utils functions here are presented some 

```{r}
mappings <- get_mappings(trans)

mappings$to_old[1:4]

mappings$to_new[1:4]

mapp_p <- cat_apply_freq(mappings$to_old, get_freqs(occup$code4[occup$year == "2008"], occup$multipier[occup$year == "2008"]))

data.frame(I(mappings$to_old), I(mapp_p)) %>% head()

mapp_p <- cat_apply_freq(mappings$to_new, get_freqs(occup$code4[occup$year == "2010"], occup$multipier[occup$year == "2010"]))

data.frame(I(mappings$to_new), I(mapp_p)) %>% head()
```

\newpage

### cat2cat applied for occup data panel

splitting the data

```{r}
occup_old = occup[occup$year == 2008,]
occup_new = occup[occup$year == 2010,]
```

simple model where probabilities will be taken from a category freqency.

```{r}
cat2cat(
  data = list(old = occup_old ,new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "forward")
  )
```

### ml module

Curentlly only knn method is availble.

```{r}
occup_2 = cat2cat(
  data = list(old = occup_old ,new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "forward")
  ,ml = list(method = "knn", features = c("age", "sex", "edu", "exp", "parttime", "salary"), args = list(k = 10))
  )
```

```{r}
occup_2 %>% glimpse()
```


```{r, size="tiny"}
occup_3 <- cat2cat(
  data = list(old = occup_old, new = occup_new, cat_var = "code", time_var = "year"),
  mappings = list(trans = trans, direction = "backward"),
  ml = list(method = "knn", features = c("age", "sex", "edu", "exp", "parttime", "salary"), args = list(k = 10))
)
```


notice : if the ml model is failing then NA is returnd so 

cor(occup_3$old$wei_ml_c2c, occup_3$old$wei_freq_c2c, use = "complete.obs")


```{r}
occup_3 %>% glimpse()
```

New freqs for potential next step:

```{r}
get_freqs(x = occup_3$new$g_new_c2c, multipier = floor(occup_3$new$multipier * occup_3$new$wei_freq_c2c)) %>% head()
```

or backward:

```{r}
get_freqs(x = occup_2$old$g_new_c2c, multipier = floor(occup_2$old$multipier * occup_2$old$wei_freq_c2c))%>% head()
```

\newpage

### Example - linear regressions:

Remember to correct degrees of freedom


### providing mappings manually

example of aggragate dataset where we want to specify transitions manually:

Simulate the data.

```{r}
set.seed(1234)

agg_old <- data.frame(
  vertical = c("Electronics", "Kids1", "Kids2", "Automotive", "Books", "Clothes", "Home", "Fashion", "Health", "Sport"),
  sales = rnorm(10, 100, 10),
  counts = rgeom(10, 0.0001),
  v_date = rep("2020-04-01", 10), stringsAsFactors = F
)

agg_new <- data.frame(
  vertical = c("Electronics", "Supermarket", "Kids", "Automotive1", "Automotive2", "Books", "Clothes", "Home", "Fashion", "Health", "Sport"),
  sales = rnorm(11, 100, 10),
  counts = rgeom(11, 0.0001),
  v_date = rep("2020-05-01", 11), stringsAsFactors = F
)
```

```{r}
agg_old %>% glimpse()
```

```{r}
agg_old %>% glimpse()
```


Remember that after that you have to aggrage the results in a proper way. 
Moreover each analysis could requires specific aggregation process.


```{r}
# old -> new,new, old-> old,new
agg1 = cat2cat_man(data = list(old = agg_old, new = agg_new, cat_var = "vertical", time_var = "v_date",freq_var = "counts"),
            Automotive %>% c(Automotive1, Automotive2), c(Kids1, Kids2) %>% c(Kids), Home %>% c(Home, Supermarket))

agg1$old
```

```{r}
agg1$new %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))
```

changes in both direction.

```{r}
agg2 = cat2cat_man(data = list(old = agg_old, new = agg_new, cat_var = "vertical", time_var = "v_date", freq_var = "counts"), 
            Automotive %<% c(Automotive1, Automotive2), c(Kids1, Kids2) %>% c(Kids), Home %>% c(Home, Supermarket))
```

```{r}
agg2$old %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))

agg2$new %>% group_by(vertical) %>% summarise(sales = sum(sales*prop), counts = sum(counts*prop), v_date = first(v_date))

```


